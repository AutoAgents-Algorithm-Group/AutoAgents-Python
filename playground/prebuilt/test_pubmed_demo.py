from time import sleep
import os
import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../')))


from src.autoagents_cua.browser import WebOperator
from src.autoagents_cua.browser import PageExtractor
from src.autoagents_cua.utils.logging import logger


class PubMedCrawler:
    """PubMed Central çˆ¬è™«"""
    
    def __init__(self, headless=False):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            headless: æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼
        """
        self.operator = WebOperator(headless=headless)
        self.extractor = PageExtractor(self.operator.page)
        self.results = []
    
    def search(self, query):
        """
        åœ¨ PubMed Central æœç´¢
        
        Args:
            query: æœç´¢å…³é”®è¯
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        try:
            # 1. æ‰“å¼€ PubMed Central
            logger.info("=" * 60)
            logger.info("æ­¥éª¤ 1: æ‰“å¼€ PubMed Central ç½‘ç«™")
            logger.info("=" * 60)
            
            success = self.operator.navigate("https://pmc.ncbi.nlm.nih.gov/", wait_time=3)
            if not success:
                logger.error("æ‰“å¼€ç½‘ç«™å¤±è´¥")
                return False
            
            # 2. ç­‰å¾…é¡µé¢åŠ è½½å¹¶æå–å…ƒç´ 
            sleep(2)
            logger.info("\næ­£åœ¨åˆ†æé¡µé¢å…ƒç´ ...")
            elements = self.extractor.extract_elements(highlight=True, save_to_file="pubmed_homepage.txt")
            logger.info(f"æ‰¾åˆ° {len(elements)} ä¸ªå¯äº¤äº’å…ƒç´ ")
            
            # 3. æŸ¥æ‰¾æœç´¢æ¡†
            logger.info("\n" + "=" * 60)
            logger.info("æ­¥éª¤ 2: è¾“å…¥æœç´¢å…³é”®è¯")
            logger.info("=" * 60)
            
            # PMC æœç´¢æ¡†çš„å®šä½å™¨ï¼ˆæ ¹æ®é¡µé¢åˆ†æï¼‰
            search_selectors = [
                'css:#pmc-search',  # PMC ä¸»æœç´¢æ¡† ID
                'css:input[name="term"]',  # æœç´¢æ¡† name
                'css:input[placeholder*="Search PMC"]',  # placeholder åŒ…å« Search PMC
                'css:input[type="search"]',  # æœç´¢æ¡† type
            ]
            
            search_box = None
            for selector in search_selectors:
                search_box = self.operator.input_text(selector, query, clear=True)
                if search_box:
                    logger.success(f"âœ… æ‰¾åˆ°æœç´¢æ¡†: {selector}")
                    break
            
            if not search_box:
                logger.error("âŒ æœªæ‰¾åˆ°æœç´¢æ¡†")
                return False
            
            logger.success(f"âœ… å·²è¾“å…¥æœç´¢å…³é”®è¯: {query}")
            sleep(1)
            
            # 4. ç‚¹å‡»æœç´¢æŒ‰é’®æˆ–æŒ‰å›è½¦
            logger.info("\næäº¤æœç´¢...")
            
            # è®°å½•å½“å‰URLï¼Œç”¨äºåˆ¤æ–­é¡µé¢æ˜¯å¦è·³è½¬
            current_url = self.operator.page.url
            search_submitted = False
            
            # æ–¹æ³•1: å°è¯•åœ¨è¾“å…¥æ¡†ä¸­æŒ‰å›è½¦ï¼ˆæ¨¡æ‹Ÿè¾“å…¥ Enter é”®ï¼‰
            if search_box:
                try:
                    logger.info("å°è¯•åœ¨è¾“å…¥æ¡†ä¸­æŒ‰å›è½¦...")
                    search_box.input('\n')  # è¾“å…¥æ¢è¡Œç¬¦è§¦å‘æäº¤
                    
                    # ç­‰å¾…é¡µé¢è·³è½¬ï¼ˆæœ€å¤šç­‰å¾…5ç§’ï¼‰
                    for i in range(10):
                        sleep(0.5)
                        if self.operator.page.url != current_url:
                            logger.success("âœ… é¡µé¢å·²è·³è½¬åˆ°æœç´¢ç»“æœé¡µ")
                            search_submitted = True
                            break
                except Exception as e:
                    logger.warning(f"è¾“å…¥å›è½¦å¤±è´¥: {e}")
            
            # æ–¹æ³•2: å°è¯•ç‚¹å‡»"Search in PMC"æŒ‰é’®ï¼ˆæœ€å¯é çš„æ–¹æ³•ï¼‰
            if not search_submitted:
                try:
                    logger.info("å°è¯•ç‚¹å‡» 'Search in PMC' æŒ‰é’®...")
                    # æ ¹æ®æŒ‰é’®æ–‡æœ¬æŸ¥æ‰¾
                    submit_button = self.operator.page.ele('css:button[type="submit"]', timeout=2)
                    if submit_button and 'Search in PMC' in submit_button.text:
                        submit_button.click()
                        logger.info("å·²ç‚¹å‡» 'Search in PMC' æŒ‰é’®")
                        
                        # ç­‰å¾…é¡µé¢è·³è½¬
                        for i in range(10):
                            sleep(0.5)
                            new_url = self.operator.page.url
                            # ç¡®ä¿è·³è½¬åˆ° PMC æœç´¢ç»“æœé¡µ
                            if new_url != current_url and 'pmc.ncbi.nlm.nih.gov' in new_url:
                                logger.success("âœ… é¡µé¢å·²è·³è½¬åˆ° PMC æœç´¢ç»“æœé¡µ")
                                search_submitted = True
                                break
                except Exception as e:
                    logger.warning(f"ç‚¹å‡»æŒ‰é’®å¤±è´¥: {e}")
            
            # æ–¹æ³•3: å°è¯•ä½¿ç”¨JavaScriptæäº¤è¡¨å•
            if not search_submitted:
                try:
                    logger.info("å°è¯•ä½¿ç”¨JavaScriptæäº¤è¡¨å•...")
                    # æŸ¥æ‰¾ PMC æœç´¢è¡¨å•å¹¶æäº¤
                    self.operator.page.run_js("""
                        let searchInput = document.querySelector('#pmc-search') || 
                                         document.querySelector('input[name="term"]');
                        if (searchInput && searchInput.form) {
                            searchInput.form.submit();
                        }
                    """)
                    
                    # ç­‰å¾…é¡µé¢è·³è½¬
                    for i in range(10):
                        sleep(0.5)
                        new_url = self.operator.page.url
                        if new_url != current_url and 'pmc.ncbi.nlm.nih.gov' in new_url:
                            logger.success("âœ… é¡µé¢å·²è·³è½¬åˆ° PMC æœç´¢ç»“æœé¡µ")
                            search_submitted = True
                            break
                except Exception as e:
                    logger.warning(f"JavaScriptæäº¤å¤±è´¥: {e}")
            
            # æ–¹æ³•4: æœ€åçš„å¤‡ç”¨æ–¹æ¡ˆ - ç›´æ¥æ„é€  PMC æœç´¢URL
            if not search_submitted:
                logger.info("å°è¯•ç›´æ¥æ„é€  PMC æœç´¢URL...")
                import urllib.parse
                # PMC çš„æ­£ç¡®æœç´¢URLæ ¼å¼
                search_url = f"https://pmc.ncbi.nlm.nih.gov/?term={urllib.parse.quote(query)}"
                if self.operator.navigate(search_url, wait_time=3):
                    new_url = self.operator.page.url
                    if 'pmc.ncbi.nlm.nih.gov' in new_url:
                        search_submitted = True
                        logger.success("âœ… å·²é€šè¿‡URLå¯¼èˆªåˆ° PMC æœç´¢ç»“æœé¡µ")
            
            if not search_submitted:
                logger.error("âŒ æœç´¢æäº¤å¤±è´¥")
                return False
            
            # 5. ç­‰å¾…æœç´¢ç»“æœåŠ è½½
            sleep(3)
            logger.success("âœ… æœç´¢ç»“æœå·²åŠ è½½")
            
            # ä¿å­˜æœç´¢ç»“æœé¡µé¢å…ƒç´ 
            self.extractor.extract_elements(highlight=True, save_to_file="pubmed_search_results.txt")
            
            return True
            
        except Exception as e:
            logger.error(f"æœç´¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def extract_article_info(self, article_index):
        """
        æå–æ–‡ç« ä¿¡æ¯ï¼ˆæ ‡é¢˜ã€ä½œè€…ã€æ‘˜è¦ï¼‰
        
        Args:
            article_index: æ–‡ç« åºå·ï¼ˆç”¨äºæ—¥å¿—ï¼‰
            
        Returns:
            dict: æ–‡ç« ä¿¡æ¯å­—å…¸
        """
        try:
            logger.info("\n" + "=" * 60)
            logger.info(f"æå–æ–‡ç«  #{article_index} çš„ä¿¡æ¯")
            logger.info("=" * 60)
            
            sleep(2)  # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
            
            article_info = {
                'index': article_index,
                'title': None,
                'authors': None,
                'abstract': None
            }
            
            # 1. æå–æ ‡é¢˜
            logger.info("\nğŸ“– æå–æ ‡é¢˜...")
            title_selectors = [
                'css:h1.content-title',
                'css:.article-title',
                'css:h1[class*="title"]',
                'css:.title-content',
                'tag:h1',
            ]
            
            for selector in title_selectors:
                title = self.operator.get_element_text(selector)
                if title and len(title) > 10:  # æ ‡é¢˜é€šå¸¸è¾ƒé•¿
                    article_info['title'] = title.strip()
                    logger.success(f"âœ… æ ‡é¢˜: {article_info['title'][:100]}...")
                    break
            
            if not article_info['title']:
                logger.warning("âš ï¸  æœªæ‰¾åˆ°æ ‡é¢˜")
            
            # 2. æå–ä½œè€…
            logger.info("\nğŸ‘¥ æå–ä½œè€…...")
            author_selectors = [
                'css:.authors',
                'css:.article-authors',
                'css:.contrib-group',
                'css:[class*="author"]',
                'css:.fm-author',
            ]
            
            for selector in author_selectors:
                authors = self.operator.get_element_text(selector)
                if authors and len(authors) > 3:
                    article_info['authors'] = authors.strip()
                    logger.success(f"âœ… ä½œè€…: {article_info['authors'][:150]}...")
                    break
            
            if not article_info['authors']:
                logger.warning("âš ï¸  æœªæ‰¾åˆ°ä½œè€…ä¿¡æ¯")
            
            # 3. æå–æ‘˜è¦
            logger.info("\nğŸ“„ æå–æ‘˜è¦...")
            abstract_selectors = [
                'css:#abstract',
                'css:.abstract',
                'css:[class*="abstract"]',
                'css:.article-abstract',
                'css:#Abs1',
            ]
            
            for selector in abstract_selectors:
                abstract = self.operator.get_element_text(selector)
                if abstract and len(abstract) > 50:  # æ‘˜è¦é€šå¸¸è¾ƒé•¿
                    article_info['abstract'] = abstract.strip()
                    logger.success(f"âœ… æ‘˜è¦: {article_info['abstract'][:200]}...")
                    break
            
            if not article_info['abstract']:
                logger.warning("âš ï¸  æœªæ‰¾åˆ°æ‘˜è¦")
            
            # ä¿å­˜æ–‡ç« é¡µé¢å…ƒç´ 
            self.extractor.extract_elements(
                highlight=True, 
                save_to_file=f"pubmed_article_{article_index}.txt"
            )
            
            return article_info
            
        except Exception as e:
            logger.error(f"æå–æ–‡ç« ä¿¡æ¯å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def crawl_search_results(self, max_results=2):
        """
        çˆ¬å–æœç´¢ç»“æœ
        
        Args:
            max_results: æœ€å¤šçˆ¬å–å‡ ç¯‡æ–‡ç« 
            
        Returns:
            list: æ–‡ç« ä¿¡æ¯åˆ—è¡¨
        """
        try:
            for i in range(1, max_results + 1):
                logger.info("\n" + "=" * 60)
                logger.info(f"æ­¥éª¤ {2 + i}: å¤„ç†ç¬¬ {i} ä¸ªæœç´¢ç»“æœ")
                logger.info("=" * 60)
                
                # è®°å½•å½“å‰URLï¼Œç”¨äºåˆ¤æ–­æ˜¯å¦æˆåŠŸè·³è½¬
                search_page_url = self.operator.page.url
                
                # 1. æŸ¥æ‰¾å¹¶ç‚¹å‡»ç¬¬ i ä¸ªæœç´¢ç»“æœ
                logger.info(f"\nğŸ” æŸ¥æ‰¾ç¬¬ {i} ä¸ªæœç´¢ç»“æœ...")
                
                # å…ˆç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
                sleep(2)
                
                # PubMed æœç´¢ç»“æœé€šå¸¸åœ¨ä¸€ä¸ªåˆ—è¡¨ä¸­
                result_selectors = [
                    f'css:.search-results article:nth-of-type({i}) a',
                    f'css:.results-list .result-item:nth-of-type({i}) a',
                    f'css:.search-result:nth-of-type({i}) a.article-title',
                    f'css:.rslt:nth-of-type({i}) a',
                    f'css:article:nth-of-type({i}) .title a',
                    f'css:.docsum-title:nth-of-type({i})',
                ]
                
                clicked = False
                article_url = None
                
                for selector in result_selectors:
                    try:
                        element = self.operator.page.ele(selector, timeout=2)
                        if element:
                            # å…ˆè·å–é“¾æ¥URLï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                            if element.tag == 'a':
                                article_url = element.attr('href')
                            
                            # ç‚¹å‡»å…ƒç´ 
                            element.click()
                            logger.info(f"å·²ç‚¹å‡»å…ƒç´ : {selector}")
                            
                            # ç­‰å¾…é¡µé¢è·³è½¬
                            for j in range(10):
                                sleep(0.5)
                                current_url = self.operator.page.url
                                if current_url != search_page_url:
                                    logger.success(f"âœ… å·²è·³è½¬åˆ°ç¬¬ {i} ä¸ªæ–‡ç« é¡µé¢")
                                    clicked = True
                                    break
                            
                            if clicked:
                                break
                    except Exception as e:
                        continue
                
                # å¤‡ç”¨æ–¹æ¡ˆï¼šå¦‚æœç‚¹å‡»å¤±è´¥ï¼Œå°è¯•ç›´æ¥å¯¼èˆª
                if not clicked and article_url:
                    logger.info(f"å°è¯•ç›´æ¥å¯¼èˆªåˆ°æ–‡ç« é¡µé¢: {article_url}")
                    if not article_url.startswith('http'):
                        # ç›¸å¯¹é“¾æ¥ï¼Œéœ€è¦è¡¥å…¨
                        base_url = "https://pmc.ncbi.nlm.nih.gov"
                        article_url = base_url + article_url
                    
                    if self.operator.navigate(article_url, wait_time=3):
                        clicked = True
                        logger.success(f"âœ… å·²å¯¼èˆªåˆ°ç¬¬ {i} ä¸ªæ–‡ç« é¡µé¢")
                
                if not clicked:
                    logger.warning(f"âš ï¸  æœªæ‰¾åˆ°ç¬¬ {i} ä¸ªæœç´¢ç»“æœï¼Œå°è¯•æå–æ‰€æœ‰æ–‡ç« é“¾æ¥...")
                    
                    # æœ€åå¤‡ç”¨æ–¹æ¡ˆï¼šæå–æ‰€æœ‰æ–‡ç« é“¾æ¥
                    try:
                        # ä½¿ç”¨DrissionPageç›´æ¥æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
                        all_links = self.operator.page.eles('tag:a', timeout=2)
                        
                        # è¿‡æ»¤å‡ºçœŸæ­£çš„æ–‡ç« é“¾æ¥
                        article_links = []
                        skip_keywords = ['skip', 'menu', 'footer', 'header', 'login', 
                                       'accessibility', 'journal', 'about', 'guide']
                        
                        for link in all_links:
                            try:
                                href = link.attr('href') or ''
                                text = link.text.strip() if link.text else ''
                                
                                # è·³è¿‡ç©ºé“¾æ¥ã€å¯¼èˆªé“¾æ¥ç­‰
                                if not text or len(text) < 20:
                                    continue
                                if any(kw in text.lower() for kw in skip_keywords):
                                    continue
                                if not href or href.startswith('#'):
                                    continue
                                    
                                # å¯èƒ½çš„æ–‡ç« é“¾æ¥ï¼ˆåŒ…å«æ–‡ç« è·¯å¾„æˆ–çœ‹èµ·æ¥åƒæ ‡é¢˜ï¼‰
                                if '/articles/' in href or len(text) > 30:
                                    article_links.append((link, text, href))
                            except:
                                continue
                        
                        logger.info(f"æ‰¾åˆ° {len(article_links)} ä¸ªå¯èƒ½çš„æ–‡ç« é“¾æ¥")
                        
                        if i <= len(article_links):
                            link_elem, link_text, link_href = article_links[i - 1]
                            logger.info(f"å°è¯•ç‚¹å‡»ç¬¬ {i} ä¸ªæ–‡ç« : {link_text[:60]}...")
                            logger.info(f"é“¾æ¥: {link_href[:80]}...")
                            
                            # ç‚¹å‡»é“¾æ¥
                            try:
                                link_elem.click()
                                
                                # ç­‰å¾…é¡µé¢è·³è½¬
                                for j in range(10):
                                    sleep(0.5)
                                    current_url = self.operator.page.url
                                    if current_url != search_page_url:
                                        logger.success(f"âœ… å·²è·³è½¬åˆ°æ–‡ç« é¡µé¢")
                                        clicked = True
                                        break
                                
                                if not clicked:
                                    logger.warning("ç‚¹å‡»åé¡µé¢æœªè·³è½¬")
                            except Exception as e:
                                logger.error(f"ç‚¹å‡»æ–‡ç« é“¾æ¥å¤±è´¥: {e}")
                        else:
                            logger.error(f"æ‰¾åˆ°çš„æ–‡ç« é“¾æ¥ä¸è¶³ {i} ä¸ª")
                    except Exception as e:
                        logger.error(f"æå–æ–‡ç« é“¾æ¥å¤±è´¥: {e}")
                
                if not clicked:
                    logger.error(f"âŒ æ— æ³•ç‚¹å‡»ç¬¬ {i} ä¸ªæœç´¢ç»“æœ")
                    continue
                
                # 2. æå–æ–‡ç« ä¿¡æ¯
                article_info = self.extract_article_info(i)
                
                if article_info:
                    self.results.append(article_info)
                    logger.success(f"\nâœ… ç¬¬ {i} ç¯‡æ–‡ç« ä¿¡æ¯å·²æå–")
                
                # 3. è¿”å›æœç´¢ç»“æœé¡µï¼ˆå¦‚æœä¸æ˜¯æœ€åä¸€ä¸ªï¼‰
                if i < max_results:
                    logger.info(f"\nâ¬…ï¸  è¿”å›æœç´¢ç»“æœé¡µ...")
                    
                    # è®°å½•å½“å‰URL
                    article_page_url = self.operator.page.url
                    
                    # è¿”å›ä¸Šä¸€é¡µ
                    self.operator.page.back()
                    
                    # ç­‰å¾…é¡µé¢è¿”å›åˆ°æœç´¢ç»“æœé¡µ
                    for j in range(10):
                        sleep(0.5)
                        current_url = self.operator.page.url
                        if current_url != article_page_url and current_url == search_page_url:
                            logger.success("âœ… å·²è¿”å›æœç´¢ç»“æœé¡µ")
                            break
                    
                    # é¢å¤–ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
                    sleep(2)
            
            return self.results
            
        except Exception as e:
            logger.error(f"çˆ¬å–æœç´¢ç»“æœå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return self.results
    
    def save_results(self, filename="pubmed_results.txt"):
        """
        ä¿å­˜çˆ¬å–ç»“æœ
        
        Args:
            filename: ä¿å­˜æ–‡ä»¶å
        """
        try:
            # è·å–è¾“å‡ºç›®å½•
            current_dir = os.path.dirname(os.path.abspath(__file__))
            output_dir = os.path.join(current_dir, 'outputs')
            os.makedirs(output_dir, exist_ok=True)
            
            output_path = os.path.join(output_dir, filename)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write("=" * 80 + "\n")
                f.write("PubMed Central çˆ¬å–ç»“æœ\n")
                f.write("æœç´¢å…³é”®è¯: characterization of solid superlubrication\n")
                f.write("=" * 80 + "\n\n")
                
                for article in self.results:
                    f.write(f"\n{'=' * 80}\n")
                    f.write(f"æ–‡ç«  #{article['index']}\n")
                    f.write(f"{'=' * 80}\n\n")
                    
                    f.write(f"ğŸ“– æ ‡é¢˜:\n{article['title']}\n\n")
                    f.write(f"ğŸ‘¥ ä½œè€…:\n{article['authors']}\n\n")
                    f.write(f"ğŸ“„ æ‘˜è¦:\n{article['abstract']}\n\n")
            
            logger.success(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")
    
    def close(self):
        """å…³é—­æµè§ˆå™¨"""
        self.operator.close()


def main():
    """ä¸»å‡½æ•°"""
    logger.info("=" * 80)
    logger.info("ğŸš€ PubMed Central è®ºæ–‡çˆ¬å–ä»»åŠ¡å¼€å§‹")
    logger.info("=" * 80)
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹ï¼ˆä¸ä½¿ç”¨æ— å¤´æ¨¡å¼ï¼Œæ–¹ä¾¿è§‚å¯Ÿï¼‰
    crawler = PubMedCrawler(headless=False)
    
    try:
        # 1. æœç´¢
        search_query = "characterization of solid superlubrication"
        success = crawler.search(search_query)
        
        if not success:
            logger.error("âŒ æœç´¢å¤±è´¥ï¼Œä»»åŠ¡ç»ˆæ­¢")
            return
        
        # 2. çˆ¬å–å‰ä¸¤ä¸ªæœç´¢ç»“æœ
        results = crawler.crawl_search_results(max_results=2)
        
        # 3. æ˜¾ç¤ºç»“æœ
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ“Š çˆ¬å–ç»“æœæ±‡æ€»")
        logger.info("=" * 80)
        
        for article in results:
            logger.info(f"\næ–‡ç«  #{article['index']}:")
            logger.info(f"  æ ‡é¢˜: {article['title'][:80] if article['title'] else 'N/A'}...")
            logger.info(f"  ä½œè€…: {article['authors'][:80] if article['authors'] else 'N/A'}...")
            logger.info(f"  æ‘˜è¦: {article['abstract'][:80] if article['abstract'] else 'N/A'}...")
        
        # 4. ä¿å­˜ç»“æœ
        crawler.save_results()
        
        logger.info("\n" + "=" * 80)
        logger.info("âœ… ä»»åŠ¡å®Œæˆï¼")
        logger.info("=" * 80)
        
        input("\næŒ‰ Enter é”®å…³é—­æµè§ˆå™¨...")
        
    except Exception as e:
        logger.error(f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        
        input("\næŒ‰ Enter é”®å…³é—­æµè§ˆå™¨...")
    
    finally:
        crawler.close()


if __name__ == "__main__":
    main()


