"""
PlanNode - ä»»åŠ¡è§„åˆ’èŠ‚ç‚¹
"""

from typing import List, Optional
from langchain_core.messages import HumanMessage, SystemMessage
from loguru import logger
import json


class PlanNode:
    """
    ä»»åŠ¡è§„åˆ’èŠ‚ç‚¹
    
    èŒè´£ï¼š
    - å°†ä»»åŠ¡åˆ†è§£ä¸ºå¯æ‰§è¡Œæ­¥éª¤
    - ä¸ºæ¯ä¸ªæ­¥éª¤å®šä¹‰å®Œæˆæ ‡å‡†
    """
    
    def __init__(
        self,
        llm: Optional[object] = None,
        default_steps: int = 3
    ):
        """
        åˆå§‹åŒ–è§„åˆ’èŠ‚ç‚¹
        
        Args:
            llm: ChatClient å®ä¾‹ï¼ˆå¯é€‰ï¼‰
            default_steps: é»˜è®¤ç”Ÿæˆçš„æ­¥éª¤æ•°
        """
        self.llm_client = llm
        self.default_steps = default_steps
    
    def __call__(self, state: dict) -> dict:
        """
        æ‰§è¡Œè§„åˆ’é€»è¾‘
        
        Args:
            state: AgentState å­—å…¸
            
        Returns:
            æ›´æ–°åçš„ state
        """
        log = logger.bind(category="Plan")
        log.info(f"ğŸ“‹ è¿›å…¥ä»»åŠ¡è§„åˆ’èŠ‚ç‚¹")
        log.debug(f"æ”¶åˆ°çš„ state keys: {list(state.keys())}")
        log.debug(f"state ä¸­æ˜¯å¦å·²æœ‰ plan: {'plan' in state}")
        
        goal = state.get('clarified_input') or state['user_input']
        log.info(f"ğŸ¯ ç›®æ ‡ä»»åŠ¡: {goal[:100]}...")
        
        generated_plan = self.generate_plan(goal)
        log.debug(f"ç”Ÿæˆçš„è®¡åˆ’: {generated_plan}")
        
        state['plan'] = generated_plan
        log.success(f"âœ… ç”Ÿæˆè®¡åˆ’ï¼Œå…± {len(state['plan'])} ä¸ªæ­¥éª¤:")
        for i, step in enumerate(state['plan'], 1):
            log.info(f"   æ­¥éª¤{i}: {step.get('desc', step)[:80]}...")
        
        state['current_step'] = 0
        state['step_tool_calls'] = 0
        state['step_tool_results'] = []
        state['no_progress_streak'] = 0
        state['hints_by_step'] = {}
        return state
    
    def generate_plan(self, clarified: str) -> List[dict]:
        """ç”Ÿæˆæ‰§è¡Œè®¡åˆ’"""
        if not self.llm_client:
            # é»˜è®¤è®¡åˆ’
            return [
                {"desc": f"æ£€ç´¢/æ”¶é›†ï¼š{clarified}", "acceptance": "è‡³å°‘3ä¸ªç‹¬ç«‹å¯é æ¥æº"},
                {"desc": "å½’çº³æ•´åˆæˆç»“æ„åŒ–è¦ç‚¹", "acceptance": "è¦†ç›–ä¸»è¦é—®é¢˜çš„è¦ç‚¹æ¸…å•"},
                {"desc": "æ’°å†™é¢å‘ç”¨æˆ·çš„å¯äº¤ä»˜æ€»ç»“", "acceptance": "å¯è¯»/æœ‰å¼•ç”¨/æœ‰ç»“è®º"}
            ]
        
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä»»åŠ¡è§„åˆ’ä¸“å®¶ã€‚æ ¹æ®ç”¨æˆ·çš„ä»»åŠ¡ç›®æ ‡ï¼Œç”Ÿæˆä¸€ä¸ªè¯¦ç»†çš„æ‰§è¡Œè®¡åˆ’ã€‚

è®¡åˆ’åº”è¯¥åŒ…å« 3-5 ä¸ªæ­¥éª¤ï¼Œæ¯ä¸ªæ­¥éª¤éƒ½è¦æœ‰ï¼š
1. desc: æ­¥éª¤çš„å…·ä½“æè¿°
2. acceptance: è¯¥æ­¥éª¤çš„å®Œæˆæ ‡å‡†

è¯·ä»¥ JSON æ ¼å¼è¿”å›ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
[
    {"desc": "æ­¥éª¤1æè¿°", "acceptance": "å®Œæˆæ ‡å‡†1"},
    {"desc": "æ­¥éª¤2æè¿°", "acceptance": "å®Œæˆæ ‡å‡†2"},
    ...
]

åªè¿”å› JSON æ•°ç»„ï¼Œä¸è¦æœ‰å…¶ä»–å†…å®¹ã€‚"""
        
        user_prompt = f"ä»»åŠ¡ç›®æ ‡ï¼š{clarified}"
        response = self._call_llm(system_prompt, user_prompt)
        
        try:
            # æå– JSONï¼ˆå¯èƒ½åŒ…è£¹åœ¨ markdown ä»£ç å—ä¸­ï¼‰
            json_str = response.strip()
            if "```json" in json_str:
                json_str = json_str.split("```json")[1].split("```")[0].strip()
            elif "```" in json_str:
                json_str = json_str.split("```")[1].split("```")[0].strip()
            
            plan_data = json.loads(json_str)
            return plan_data
        except Exception as e:
            print(f"è§£æè®¡åˆ’å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤è®¡åˆ’")
            return [
                {"desc": f"æ£€ç´¢/æ”¶é›†ç›¸å…³ä¿¡æ¯ï¼š{clarified}", "acceptance": "è‡³å°‘3ä¸ªç‹¬ç«‹å¯é æ¥æº"},
                {"desc": "å½’çº³æ•´åˆæˆç»“æ„åŒ–è¦ç‚¹", "acceptance": "è¦†ç›–ä¸»è¦é—®é¢˜çš„è¦ç‚¹æ¸…å•"},
                {"desc": "æ’°å†™é¢å‘ç”¨æˆ·çš„å¯äº¤ä»˜æ€»ç»“", "acceptance": "å¯è¯»/æœ‰å¼•ç”¨/æœ‰ç»“è®º"}
            ]
    
    def _call_llm(self, system_prompt: str, user_prompt: str, temperature: float = 0.7) -> str:
        """è°ƒç”¨ LLM å¹¶è¿”å›å“åº”æ–‡æœ¬"""
        if not self.llm_client:
            return f"[æ¨¡æ‹Ÿå“åº”: {user_prompt[:50]}...]"
        
        try:
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=user_prompt)
            ]
            response = self.llm_client.llm.invoke(messages)
            return response.content
        except Exception as e:
            print(f"LLM è°ƒç”¨å¤±è´¥: {e}")
            return f"[LLMè°ƒç”¨å¤±è´¥: {str(e)}]"

