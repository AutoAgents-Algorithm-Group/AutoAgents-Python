"""
ClarifyNode - ä»»åŠ¡æ¾„æ¸…èŠ‚ç‚¹
"""

from typing import Optional
from langchain_core.messages import HumanMessage, SystemMessage
from loguru import logger
from ..prompts import prompt_loader


class ClarifyNode:
    """
    ä»»åŠ¡æ¾„æ¸…èŠ‚ç‚¹
    
    èŒè´£ï¼š
    - åˆ¤æ–­ä»»åŠ¡æè¿°æ˜¯å¦æ¸…æ™°
    - ç”Ÿæˆæ¾„æ¸…é—®é¢˜
    - æ•´åˆç”¨æˆ·è¡¥å……ä¿¡æ¯
    """
    
    def __init__(
        self,
        llm: Optional[object] = None,
        max_clarify_rounds: int = 1,
        min_query_length: int = 30
    ):
        """
        åˆå§‹åŒ–æ¾„æ¸…èŠ‚ç‚¹
        
        Args:
            llm: ChatClient å®ä¾‹ï¼ˆå¯é€‰ï¼‰
            max_clarify_rounds: æœ€å¤§æ¾„æ¸…è½®æ¬¡ï¼ˆé»˜è®¤1æ¬¡ï¼Œå¼ºåˆ¶æ¾„æ¸…ï¼‰
            min_query_length: åˆ¤æ–­æŸ¥è¯¢æ˜¯å¦æ¸…æ™°çš„æœ€å°é•¿åº¦
        """
        self.llm_client = llm
        self.max_clarify_rounds = max_clarify_rounds
        self.min_query_length = min_query_length
    
    def __call__(self, state: dict) -> dict:
        """
        æ‰§è¡Œæ¾„æ¸…é€»è¾‘
        
        Args:
            state: AgentState å­—å…¸
            
        Returns:
            æ›´æ–°åçš„ state
        """
        log = logger.bind(category="Clarify")
        log.info(f"ğŸ“ è¿›å…¥ä»»åŠ¡æ¾„æ¸…èŠ‚ç‚¹ (è½®æ¬¡: {state['clarify_count']}/{self.max_clarify_rounds})")
        
        if state['clarify_count'] >= self.max_clarify_rounds:
            log.info(f"âœ… å·²è¾¾æœ€å¤§æ¾„æ¸…è½®æ¬¡")
            if not state.get('clarified_input'):
                state['clarified_input'] = state['user_input']
            return state
        
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç”¨æˆ·æ¾„æ¸…å›å¤
        if state.get('user_clarification'):
            # ç”¨æˆ·å·²æä¾›æ¾„æ¸…ï¼Œæ•´åˆå›å¤
            log.info(f"âœ… æ”¶åˆ°ç”¨æˆ·æ¾„æ¸…: {state['user_clarification'][:50]}...")
            state['clarified_input'] = self.incorporate_user_response(
                state['user_input'], 
                state['user_clarification']
            )
            state['clarify_count'] += 1
            state['needs_clarification'] = False
            state['clarification_question'] = None
            state['user_clarification'] = None
            log.info(f"âœ… æ¾„æ¸…å®Œæˆï¼Œæ›´æ–°åçš„ä»»åŠ¡: {state['clarified_input'][:100]}...")
            return state
        
        # å¼ºåˆ¶è¿›è¡Œæ¾„æ¸…ï¼šç”Ÿæˆé—®é¢˜å¹¶ç­‰å¾…ç”¨æˆ·è¾“å…¥
        log.info(f"ğŸ”„ ç”Ÿæˆæ¾„æ¸…é—®é¢˜...")
        q = self.generate_clarification_question(state['user_input'])
        log.info(f"â“ æ¾„æ¸…é—®é¢˜: {q[:200]}...")
        log.warning(f"â¸ï¸ ç­‰å¾…ç”¨æˆ·æ¾„æ¸…...")
        
        state['needs_clarification'] = True
        state['clarification_question'] = q
        return state
    
    def is_query_clear(self, query: Optional[str]) -> bool:
        """åˆ¤æ–­æŸ¥è¯¢æ˜¯å¦æ¸…æ™°"""
        if not query or len(query.strip()) < 10:
            return False
        
        if not self.llm_client:
            # ç®€å•åˆ¤æ–­ï¼šé•¿åº¦å¤Ÿé•¿å°±è®¤ä¸ºæ¸…æ™°
            return len(query.strip()) >= self.min_query_length
        
        # ä½¿ç”¨ LLM åˆ¤æ–­ï¼ˆä» Markdown åŠ è½½æç¤ºè¯ï¼‰
        prompt = prompt_loader.load("clarify/is_query_clear.md", query=query)
        response = self._call_llm_with_prompt(prompt, temperature=0.0)
        return "CLEAR" in response.upper()
    
    def generate_clarification_question(self, query: str) -> str:
        """ç”Ÿæˆæ¾„æ¸…é—®é¢˜"""
        if not self.llm_client:
            return f"ä¸ºå®Œæˆä»»åŠ¡ï¼Œè¯·è¡¥å……å…³é”®çº¦æŸ/èŒƒå›´/äº¤ä»˜ï¼š{query}"
        
        # ä» Markdown åŠ è½½æç¤ºè¯ï¼ˆæ”¯æŒå˜é‡æ’å…¥ï¼‰
        prompt = prompt_loader.load("clarify/generate_question.md", query=query)
        return self._call_llm_with_prompt(prompt)
    
    def incorporate_user_response(self, query: str, user_resp: str) -> str:
        """æ•´åˆç”¨æˆ·çš„è¡¥å……ä¿¡æ¯"""
        return f"{query} | æ¾„æ¸…è¡¥å……ï¼š{user_resp}"
    
    def _call_llm(self, system_prompt: str, user_prompt: str, temperature: float = 0.7) -> str:
        """è°ƒç”¨ LLM å¹¶è¿”å›å“åº”æ–‡æœ¬ï¼ˆä¼ ç»Ÿæ–¹å¼ï¼šsystem + userï¼‰"""
        if not self.llm_client:
            return f"[æ¨¡æ‹Ÿå“åº”: {user_prompt[:50]}...]"
        
        try:
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=user_prompt)
            ]
            response = self.llm_client.llm.invoke(messages)
            return response.content
        except Exception as e:
            print(f"LLM è°ƒç”¨å¤±è´¥: {e}")
            return f"[LLMè°ƒç”¨å¤±è´¥: {str(e)}]"
    
    def _call_llm_with_prompt(self, prompt: str, temperature: float = 0.7) -> str:
        """è°ƒç”¨ LLM å¹¶è¿”å›å“åº”æ–‡æœ¬ï¼ˆä½¿ç”¨å•ä¸ªå®Œæ•´æç¤ºè¯ï¼‰"""
        if not self.llm_client:
            return f"[æ¨¡æ‹Ÿå“åº”: {prompt[:50]}...]"
        
        try:
            messages = [HumanMessage(content=prompt)]
            response = self.llm_client.llm.invoke(messages)
            return response.content
        except Exception as e:
            print(f"LLM è°ƒç”¨å¤±è´¥: {e}")
            return f"[LLMè°ƒç”¨å¤±è´¥: {str(e)}]"